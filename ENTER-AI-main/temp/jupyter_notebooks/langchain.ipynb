{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee161ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "ak = \"api-key\"\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94906a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredExcelLoader\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10e9f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35b07c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('review.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd76b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df,page_content_column='ë¦¬ë·°')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8fe355a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee7aa4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llama_embedding = LlamaCppEmbeddings(model_path=\"./lla.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf3613b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs,embedding=llama_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0994994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs,embedding=OpenAIEmbeddings(openai_api_key=ak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "419f3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab3ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62ffbec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "198f9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb47677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=ak, temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0767cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(openai_api_key=ak, temperature=0)\n",
    "retriever_from_llm2 = MultiQueryRetriever.from_llm(\n",
    "    retriever=ensemble_retriever, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eab36f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bba92630",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Guess the answer in korean about the question by referring the following context,:{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40645a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655dbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02eb0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever ì´ìš©\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(openai_api_key=ak,temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(openai_api_key=ak),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "689f7949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='ì•„ë§ˆë„ ì¹´ì´ë§‰ì€ ë§›ìˆì§€ë§Œ ê°€ê²Œì—ëŠ” ì–´ë–¤ ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'),\n",
       " 'docs': [Document(page_content='ë””ì €íŠ¸ê°€ ë§›ìˆì–´ìš”!', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 21ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì¹´ì´ë§‰ ì§„ì§œ ë§›ìˆì–´ìš”!!!', metadata={'ë‚ ì§œ': '2023ë…„ 5ì›” 12ì¼', 'ë°©ë¬¸íšŸìˆ˜': '3ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì¹´ì´ë§‰ ì§„ì§œ ë§›ìˆì–´ìš”!!!', metadata={'ë‚ ì§œ': '2023ë…„ 4ì›” 27ì¼', 'ë°©ë¬¸íšŸìˆ˜': '2ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ë§¤ë²ˆ ì¹´ì´ë§‰ ë§›ìˆê²Œ ë¨¹ëŠ” ë§›ì§‘ì´ì˜ˆìš”:)', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 22ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"ê°€ê²Œì˜ ë‹¨ì \"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cdf42fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQueryRetriever ì´ìš©\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(openai_api_key=ak,temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever_from_llm,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(openai_api_key=ak),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1f9cd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='ê°€ê²Œì˜ ê°€ì¥ ì¢‹ì€ ì ì€ ë©”ë‰´ì˜ ë‹¤ì–‘ì„±ì´ë©°, ì»¤í”¼ì™€ ì£¼ë¥˜ê°€ ëª¨ë‘ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.'),\n",
       " 'docs': [Document(page_content='ì•„ì´ìŠ¤í¬ë¦¼ í• ì¸ì ', metadata={'ë‚ ì§œ': '2023ë…„ 1ì›” 18ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì¹´ì´ë§‰ ë§›ìˆì–´ìš” ê°€ê²Œ ë¶„ìœ„ê¸°ë„ ì•„ëŠ‘í•˜êµ¬ ì¢‹ë„¤ìš”', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 8ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ë§›ìˆì–´ìš” ë°©ë¬¸í•˜ê¸°ì¢‹ì€ìœ„ì¹˜ì—ìš”. ë§¤ì¥ì´ì²­ê²°í•´ìš”', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 22ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì‚¬ì¥ë‹˜ì´ ì¹œì ˆí•˜ê³  ê°€ê²Œê°€ ê¹”ë”í•´ìš”! ì¹´ì´ë§‰ í¬ì¥ì£¼ë¬¸í•´ê°€ì§€ë§Œ ë‹¤ìŒì—ëŠ” ì¹œêµ¬ë‘ ì„¸íŠ¸ë©”ë‰´ ë¨¹ê³ ê°ˆê±°ì˜ˆìš”', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 21ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ê¹”ë”í•˜ê³  ì˜ˆìœ ë‚´ë¶€, ê·¸ë¦¬ê³  ë¬´ì—‡ë³´ë‹¤ ë©”ë‰´ê°€ ë‹¤ì–‘í•´ì„œ ì¢‹ì•„ìš”! ì»¤í”¼ ì™€ ì£¼ë¥˜ ëª¨ë‘ ê°€ëŠ¥í•œ ì ì´ ì¢‹ì•˜ì–´ìš”â¤ï¸', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 29ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='í•´ë°©ì´Œ ì´ˆì…ì— ìˆëŠ” ê¹”ë”í•œ ì¹´í˜ ë‹¬ ê°„ë‹¨í•œ ì•ˆì£¼ë‘ ë§¥ì£¼ë„ ìˆê³  ë¬´ì—‡ë³´ë‹¤ ì¹´ì´ë§‰ì´ ìˆì–´ìš”~ ì´ë²¤íŠ¸ë„ ì§„í–‰ì¤‘ì´ì—¬ì„œ ê°€ì„±ë¹„ ìˆì–´ìš”!', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 29ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'})]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"ìš°ë¦¬ ê°€ê²Œì˜ ê°€ì¥ ì¢‹ì€ ì ì´ ë­ì•¼?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ba45926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQueryRetriever ì´ìš©\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(openai_api_key=ak,temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever_from_llm2,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(openai_api_key=ak),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7512817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='ì¹´ì´ë§‰ì´ ê°€ì¥ ì¸ê¸° ìˆëŠ” ê²ƒ ê°™ì•„ìš”.'),\n",
       " 'docs': [Document(page_content='ì¹´ì´ë§‰ì´ë‘ ìŒë£Œ ë‹¤ ë§›ìˆì–´ìš” ğŸ¤¤ \\U0001fa77\\U0001fa77', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 21ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ê¹”ë”í•˜ê³  ì˜ˆìœ ë‚´ë¶€, ê·¸ë¦¬ê³  ë¬´ì—‡ë³´ë‹¤ ë©”ë‰´ê°€ ë‹¤ì–‘í•´ì„œ ì¢‹ì•„ìš”! ì»¤í”¼ ì™€ ì£¼ë¥˜ ëª¨ë‘ ê°€ëŠ¥í•œ ì ì´ ì¢‹ì•˜ì–´ìš”â¤ï¸', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 29ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ë„ˆë¬´ ë©‹ì ¸ìš” ìŒë£Œ ë§›ìˆì–´ìš”', metadata={'ë‚ ì§œ': '2023ë…„ 1ì›” 15ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì¹´ì´ë§‰ ì„¸íŠ¸ ë©”ë‰´ê°€ ì €ë ´í•´ì„œ ì¢‹ì•˜êµ¬ì—¬ ì¹´ì´ë§‰ì´ ì •ë§ ë§›ìˆì–´ìš”!  ë”¸ê¸°ìˆ˜ì œì²­ë„ êµ‰ì¥íˆ ë§›ìˆì—ˆê³  ë°”í˜•ì‹ìœ¼ë¡œ ë˜ì–´ìˆì–´ì„œ ì¼ë°˜ ì¹´í˜ë‘ ë””ìì¸ì´ ë‹¬ë¼ì„œ ì¢€ ë” ë¶„ìœ„ê¸°ìˆì–´ìš”!', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 19ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì§€ë‚˜ê°€ëŠ” ê¸¸ì— ì˜ˆë»ë³´ì—¬ì„œ ë“¤ì–´ê°”ëŠ”ë° ì»¤í”¼ ë§›ì´ì¢‹ë„¤ìš”!! ì‹œê°„ì´ ë³„ë¡œ ì—†ì–´ì„œ í…Œì´í¬ì•„ì›ƒìœ¼ë¡œ í–ˆëŠ”ë° ì‹œê°„ ë  ë•Œ ë°” ìë¦¬ì— ì•‰ì•„ì„œ ë§ˆì‹œê³  ì‹¶ì–´ìš”âœ¨â˜ºï¸', metadata={'ë‚ ì§œ': '2022ë…„ 10ì›” 30ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì‚¬ì¥ë‹˜ê»˜ì„œ ë„ˆë¬´ ì¹œì ˆí•˜ì‹œê³  ë©”ë‰´ ì„¤ëª…ì´ ìƒì„¸í•˜ê²Œ ì˜ ë˜ì–´ ìˆì–´ì„œ ë„ˆë¬´ ì¢‹ì•„ìš” :)', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 22ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ë””ì €íŠ¸ê°€ ë§›ìˆì–´ìš”!', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 21ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì¹´ì´ë§‰ ë§›ìˆì–´ìš” ê°€ê²Œ ë¶„ìœ„ê¸°ë„ ì•„ëŠ‘í•˜êµ¬ ì¢‹ë„¤ìš”', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 8ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='í•´ë°©ì´Œ ì´ˆì…ì— ìˆëŠ” ì‘ì€ ì¹´í˜ë„¤ìš”. ì¹´ì´ë§‰ê³¼ ë¼ë–¼ ë¨¹ì—ˆìŠµë‹ˆë‹¤. ì¹´ì´ë§‰ì€ ì²˜ìŒ ë¨¹ì–´ë´¤ëŠ”ë° ê³ ì†Œë‹¬ë‹¬í•˜ë„¤ìš”. ', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 22ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì¹´ì´ë§‰ ë¨¹ìœ¼ë¡œ ì™”ì–´ìš”~~ ë¶„ìœ„ê¸°ê°€ ì¢‹ë„¤ìš”\\n', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 22ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='í•´ë°©ì´Œ ì´ˆì…ì— ìˆëŠ” ê¹”ë”í•œ ì¹´í˜ ë‹¬ ê°„ë‹¨í•œ ì•ˆì£¼ë‘ ë§¥ì£¼ë„ ìˆê³  ë¬´ì—‡ë³´ë‹¤ ì¹´ì´ë§‰ì´ ìˆì–´ìš”~ ì´ë²¤íŠ¸ë„ ì§„í–‰ì¤‘ì´ì—¬ì„œ ê°€ì„±ë¹„ ìˆì–´ìš”!', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 29ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='í•´ë°©ì´Œ ì´ˆì…ì— ìˆëŠ” ë‹¬ ë°” ë„¤ìš”  ì»¤í”¼ë„ ìˆê³  ìˆ ë„ ìˆê³  ì¢‹ë„¤ìš” ì¸í…Œë¦¬ì–´ê°€ ë„ˆë¬´ ì´ì˜ê³   ê°¬ì„±ì‚¬ì§„ ê° ì´ë„¤ìš”ğŸ˜† í¬ë¦¼ì»¤í”¼ ë§›ì§‘â£ï¸ ì²«ë°©\\në¬¸ì¸ë° ë„ˆë¬´ í–‰ë³µí•œ ì‹œê°„ ë³´ë‚´ê³  ê°‘ë‹ˆë‹¤ğŸ§¡ë‚´ìš© ë”ë³´ê¸°', metadata={'ë‚ ì§œ': '2022ë…„ 11ì›” 9ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì œê°€ ë¦¬ë·°ê°™ì€ê±° ì •ë§ ì˜ ëª»ì¨ì„œ ì´ëŸ°ê±° ì˜ì•ˆí•˜ëŠ”ë° ì¹´ì´ë§‰ì„ ì²˜ìŒ ì ‘í•´ë³¼ìˆ˜ ìˆëŠ” ì¹´í˜ì˜€ëŠ”ë° ë°©ë¬¸í•´ì„œ ë¨¹ì„ì‹œê°„ì´ ì•ˆë˜ì„œ í¬ì¥ì£¼ë¬¸ì„\\ní–ˆëŠ”ë° ì¹œì ˆíˆ í¬ì¥ë„ í•´ì£¼ì…¨êµ¬ìš”~ ì§ì¥ ë™ë£Œì™€ ê°™ì´ ë¨¹ì—ˆëŠ”ë° ë™ë£Œë„ ë„ˆë¬´ ë§Œì¡±ìŠ¤ëŸ¬ì›Œ í•˜ë”ë¼ê³ ìš”~ ë‹¤ìŒì—” ë§¤ì¥ì—ì„œ ë¶„ìœ„ê¸°ìˆê²Œ ë¨¹\\nì–´ë³¼ ìƒê°ì…ë‹ˆë‹¤~ ìš°ë¦¬ë™ë„¤ë¼ì„œê°€ ì•„ë‹ˆë¼ ì—¬ê¸° ì •ë§ ë§›ìˆì–´ìš”~ ì§„ì§œ ë¦¬ë·°ê°™ì€ê±° ì•ˆì“°ëŠ”ë°... ë˜ ë¨¹ê³ ì‹¶ê³  ë˜ ë¦¬ë·° ë‚¨ê²¨ë“œë¦¬ê³  ì‹¶...ë‚´ìš© ë”ë³´ê¸°', metadata={'ë‚ ì§œ': '2023ë…„ 10ì›” 17ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'}),\n",
       "  Document(page_content='ì˜ ë¨¹ì—ˆìŠµë‹ˆë‹¤ ', metadata={'ë‚ ì§œ': '2023ë…„ 5ì›” 14ì¼', 'ë°©ë¬¸íšŸìˆ˜': '1ë²ˆì§¸ ë°©ë¬¸'})]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"ê°€ì¥ ì¸ê¸° ë§ì€ ë©”ë‰´ê°€ ë­ì•¼?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cfbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain.chat_models import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5370576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0355628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "83beb57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"./lla.bin\",\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba6174a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb9437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, let us look at the definition of \"Super Bowl\":\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "The Super Bowl is the championship game of the National Football League (NFL), played between the winners of the NFL's American Football Conference (AFC) and National Football Conference (NFC) Championships. It has been held every year since 1967, when it was first played on January 14, 1967\n",
      "  â€” the day before Super Bowl XLVIII. The game is typically the last NFL game of the season; the NFL playoffs then begin in earnest with the Wild Card Playoff games (if there are any), and the Divisional Playoff games. This is followed by the NFL Championship Game, or \"Super Bowl\" as it has become known since 1976, when the NFL Playoffs were expanded to include the AFC-NFC conference title game.\n",
      "\\end{blockquote}\n",
      "\n",
      "Now that we know what Super Bowls are and how they work, let us now take a look at the year Justin Bieber was born:\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "February 1, 1994\n",
      "\\end{blockquote}\n",
      "\n",
      "Now, let's put these two pieces of information together. If Justin was born in February 1st, then that must be a Super Bowl year. However, we know that the Super Bowls are played at the end of the season, not the beginning. So we have to look at what happened after Justin Bieber's birthday:\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "The 1993 NFL Playoffs were held from January 8 through January 22; the Super Bowl was played on Sunday January 29th, 1994. The year before that (1993) had a playoff format, but no Super Bowl. The year after Justin's birthday had no playoffs at all.\n",
      "\\end{blockquote}\n",
      "\n",
      "So the answer to our question is:\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "Super Bowel XLIV\n",
      "\\end{blockquote}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' First, let us look at the definition of \"Super Bowl\":\\n\\n\\\\begin{blockquote}\\n\\nThe Super Bowl is the championship game of the National Football League (NFL), played between the winners of the NFL\\'s American Football Conference (AFC) and National Football Conference (NFC) Championships. It has been held every year since 1967, when it was first played on January 14, 1967\\n  â€” the day before Super Bowl XLVIII. The game is typically the last NFL game of the season; the NFL playoffs then begin in earnest with the Wild Card Playoff games (if there are any), and the Divisional Playoff games. This is followed by the NFL Championship Game, or \"Super Bowl\" as it has become known since 1976, when the NFL Playoffs were expanded to include the AFC-NFC conference title game.\\n\\\\end{blockquote}\\n\\nNow that we know what Super Bowls are and how they work, let us now take a look at the year Justin Bieber was born:\\n\\n\\\\begin{blockquote}\\n\\nFebruary 1, 1994\\n\\\\end{blockquote}\\n\\nNow, let\\'s put these two pieces of information together. If Justin was born in February 1st, then that must be a Super Bowl year. However, we know that the Super Bowls are played at the end of the season, not the beginning. So we have to look at what happened after Justin Bieber\\'s birthday:\\n\\n\\\\begin{blockquote}\\n\\nThe 1993 NFL Playoffs were held from January 8 through January 22; the Super Bowl was played on Sunday January 29th, 1994. The year before that (1993) had a playoff format, but no Super Bowl. The year after Justin\\'s birthday had no playoffs at all.\\n\\\\end{blockquote}\\n\\nSo the answer to our question is:\\n\\n\\\\begin{blockquote}\\n\\nSuper Bowel XLIV\\n\\\\end{blockquote}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4437416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7c337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b4b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df13d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb961e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07d60428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(openai_api_key=ak,temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(openai_api_key=ak),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb3746f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"where do harrison want to work?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ddce417",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "006794f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Based on the context provided, Harrison wants to work in Europe.'),\n",
       " 'docs': [Document(page_content='harrison want to go europe'),\n",
       "  Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='harrison lived in korea')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf1d8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b6665cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='where do harrison want to work?'),\n",
       "  AIMessage(content='Based on the context provided, Harrison wants to work in Europe.')]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ae0e95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Based on the given context, it is unlikely that Harrison would consider working in Korea since he wants to go to Europe and currently works at Kensho.'),\n",
       " 'docs': [Document(page_content='harrison lived in korea'),\n",
       "  Document(page_content='harrison want to go europe'),\n",
       "  Document(page_content='harrison worked at kensho')]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"How about working in korea?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4211a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b281071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='where do harrison want to work?'),\n",
       "  AIMessage(content='Based on the context provided, Harrison wants to work in Europe.'),\n",
       "  HumanMessage(content='How about working in korea?'),\n",
       "  AIMessage(content='Based on the given context, it is unlikely that Harrison would consider working in Korea since he wants to go to Europe and currently works at Kensho.')]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c57cb61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Based on the given context, it would not be good for Harrison to work in Korea since he wants to go to Europe.'),\n",
       " 'docs': [Document(page_content='harrison lived in korea'),\n",
       "  Document(page_content='harrison want to go europe'),\n",
       "  Document(page_content='harrison worked at kensho')]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"He lived in korea. so I think It is good to work in korea\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174a23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b68aed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Is \"{question}\" related to cafe?\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1bede72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"./lla.bin\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c04b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {\"question\": RunnablePassthrough()} | ANSWER_PROMPT | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a1d2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What do you like best about our cafe?\n",
      "\n",
      "Human: How can we improve the coffee experience for you?\n",
      "\n",
      "Human: What is your favorite food and beverage? \n",
      "\n",
      "Human: How can we improve the food experience for you?\n",
      "\n",
      "Human: Is there any other feedback you'd like to share with us?\n",
      "\n",
      "Human: Thank You!\n",
      "\n",
      "Human: Have a nice day!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHuman: What do you like best about our cafe?\\n\\nHuman: How can we improve the coffee experience for you?\\n\\nHuman: What is your favorite food and beverage? \\n\\nHuman: How can we improve the food experience for you?\\n\\nHuman: Is there any other feedback you'd like to share with us?\\n\\nHuman: Thank You!\\n\\nHuman: Have a nice day!\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aa4aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = OpenAI(openai_api_key=\"api-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9422a97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, the comment \"coffee is delicious\" is not related to a telecommunications company.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2('Is comment that is \"coffee is delicious\" related to telecommunications company?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f0267a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"question\": RunnablePassthrough()} | ANSWER_PROMPT | llm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a06186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nComputer: It depends on the context. Taste could be related to cafe if it is referring to the taste of food served at the cafe, but it could also be unrelated if it is referring to a different type of experience.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.invoke(\"taste is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f3279af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have a question about the sentence \"Taste is good\". I know that it's a grammatical mistake, but I don't understand why.\n",
      "\n",
      "Is this sentence related to cafes?\n",
      "\n",
      "Comment: It's not a grammatical error. It's an idiomatic expression.\n",
      "\n",
      "Answer: The sentence \"Taste is good\" is not a grammatical error. It's an idiomatic expression."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nI have a question about the sentence \"Taste is good\". I know that it\\'s a grammatical mistake, but I don\\'t understand why.\\n\\nIs this sentence related to cafes?\\n\\nComment: It\\'s not a grammatical error. It\\'s an idiomatic expression.\\n\\nAnswer: The sentence \"Taste is good\" is not a grammatical error. It\\'s an idiomatic expression.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Is \"Taste is good\" related to cafe\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13837410",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: answer the question\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "23912744",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''<s>[INST] <<SYS>>\n",
    "You know well about coffee\n",
    "<</SYS>>\n",
    "\n",
    " [/INST]</s>\n",
    "<s>[INST] What is Kaymak?[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    " [/INST]</s>\n",
    "<s>[INST] What is Kaymak? [/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0eb9501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''User: What is Kaymak?\n",
    "Assistant : You know well about coffee'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "639fa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: ì»¤í”¼ëŠ” ì–´ë•Œ? Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6a6b8a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### ë¬¸ì œ 1\n",
      "\n",
      "ì‘ì€ ê´€ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•œ ì»¤í”¼ë¥¼ ë¨¹ê¸° ì „ì— ë§ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì»¤í”¼ëŠ” ì–´ë•Œ?\n",
      "\n",
      "### ë¬¸ì œ 2\n",
      "\n",
      "ì‘ì€ ê´€ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•œ ì»¤í”¼ë¥¼ ë¨¹ê¸° ì „ì— ë§ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì»¤í”¼ëŠ” ì–´ë•Œ?\n",
      "\n",
      "### ë¬¸ì œ 3\n",
      "\n",
      "ì‘ì€ ê´€ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•œ ì»¤í”¼ë¥¼ ë¨¹ê¸° ì „ì— ë§ì´ë‹¤. ê·¸"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm(prompt)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:892\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    893\u001b[0m         [prompt],\n\u001b[0;32m    894\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    895\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    896\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    897\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    898\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    899\u001b[0m     )\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    902\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     ]\n\u001b[1;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    667\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    668\u001b[0m     )\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 540\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    541\u001b[0m                 prompts,\n\u001b[0;32m    542\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    543\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    544\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    545\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    546\u001b[0m             )\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1069\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1068\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1069\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1073\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\llamacpp.py:291\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    292\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m     ):\n\u001b[0;32m    297\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\llamacpp.py:344\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    343\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    345\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    346\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    347\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    348\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[0;32m    349\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1473\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1471\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1472\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1474\u001b[0m     prompt_tokens,\n\u001b[0;32m   1475\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1476\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1477\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1478\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1479\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1480\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1481\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1482\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1483\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1484\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1485\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1486\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1487\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1488\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1489\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m   1492\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1248\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m   1249\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1250\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1251\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m   1266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   1267\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1069\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1065\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m   1067\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m   1068\u001b[0m )\n\u001b[1;32m-> 1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:469\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    470\u001b[0m     ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    471\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama_cpp.py:1475\u001b[0m, in \u001b[0;36mllama_decode\u001b[1;34m(ctx, batch)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lib\u001b[38;5;241m.\u001b[39mllama_decode(ctx, batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "50dea8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Suzuki: I think it's a good question.\n",
      "Suzuki: I think it's a good question.\n",
      "Suzuki: I think it's a good question.\n",
      "Suzuki: I think it's a good question."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIs comment that is \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoffee is delicious\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m related to telecommunications company?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:892\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    893\u001b[0m         [prompt],\n\u001b[0;32m    894\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    895\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    896\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    897\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    898\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    899\u001b[0m     )\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    902\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     ]\n\u001b[1;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    667\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    668\u001b[0m     )\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 540\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    541\u001b[0m                 prompts,\n\u001b[0;32m    542\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    543\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    544\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    545\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    546\u001b[0m             )\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1069\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1068\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1069\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1073\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\llamacpp.py:291\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    292\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m     ):\n\u001b[0;32m    297\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\llamacpp.py:344\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    343\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    345\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    346\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    347\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    348\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[0;32m    349\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1473\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1471\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1472\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1474\u001b[0m     prompt_tokens,\n\u001b[0;32m   1475\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1476\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1477\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1478\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1479\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1480\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1481\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1482\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1483\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1484\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1485\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1486\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1487\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1488\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1489\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m   1492\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1248\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m   1249\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1250\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1251\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m   1266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   1267\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1069\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1065\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m   1067\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m   1068\u001b[0m )\n\u001b[1;32m-> 1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:469\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    470\u001b[0m     ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    471\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama_cpp.py:1475\u001b[0m, in \u001b[0;36mllama_decode\u001b[1;34m(ctx, batch)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lib\u001b[38;5;241m.\u001b[39mllama_decode(ctx, batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm('Is comment that is \"coffee is delicious\" related to telecommunications company?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58336f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5130b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cc7440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Comment: I've never heard of it before, but I think it's a Turkish word for \"cream\"."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: what is Kaymak?\n",
    "\"\"\"\n",
    "response = llm_chain.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8ae1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "03b29763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llmc = LlamaCpp(\n",
    "    model_path=\"./llachat.bin\",\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f84dcc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = '''[INST] <<SYS>>\n",
    "You are a CEO of telecommunications company<</SYS>>\n",
    "'Is comment that is \"coffee is delicious\" related to telecommunications company? your answer start Yes or No'[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d640cf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As the CEO of a telecommunications company, I must say that while coffee may be delicious, it has little to no relevance to our industry. Telecommunications is all about facilitating communication and data transfer through various networks and technologies, such as cellular networks, fiber optic cables, and satellite communications.\n",
      "\n",
      "However, I do appreciate your enthusiasm for coffee! As a CEO, it's important to stay energized and focused, and a good cup of coffee can certainly help with that. Perhaps we could discuss the latest advancements in telecommunications technology over a cup of coffee sometime? ğŸ˜Š"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  As the CEO of a telecommunications company, I must say that while coffee may be delicious, it has little to no relevance to our industry. Telecommunications is all about facilitating communication and data transfer through various networks and technologies, such as cellular networks, fiber optic cables, and satellite communications.\\n\\nHowever, I do appreciate your enthusiasm for coffee! As a CEO, it's important to stay energized and focused, and a good cup of coffee can certainly help with that. Perhaps we could discuss the latest advancements in telecommunications technology over a cup of coffee sometime? ğŸ˜Š\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmc.invoke(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a42254d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No, the comment \"coffee is delicious\" is not directly related to a telecommunications company. As a CEO of a telecommunications company, my focus would be on providing high-quality communication services and solutions, rather than promoting coffee. Therefore, I would answer with a simple \"No\"."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  No, the comment \"coffee is delicious\" is not directly related to a telecommunications company. As a CEO of a telecommunications company, my focus would be on providing high-quality communication services and solutions, rather than promoting coffee. Therefore, I would answer with a simple \"No\".'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmc.invoke(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ce98799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = '''[INST] <<SYS>>\n",
    "You are a CEO of telecommunications company<</SYS>>\n",
    "'Is comment that is \"speed is too slow\" related to telecommunications company? your answer start Yes or No'[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "952e0226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As the CEO of a telecommunications company, I would say...\n",
      "\n",
      "Yes, the comment \"speed is too slow\" can be related to our company. Our customers expect fast and reliable internet speeds, and any issues with speed can impact their ability to work, stream content, or communicate with others. We take speed and reliability very seriously and invest heavily in our network infrastructure to ensure that our customers receive the best possible service. However, we understand that sometimes speed issues can arise due to various factors such as high traffic, network congestion, or technical difficulties. Our customer support team is available 24/7 to address any speed-related concerns and provide solutions to improve connectivity."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  As the CEO of a telecommunications company, I would say...\\n\\nYes, the comment \"speed is too slow\" can be related to our company. Our customers expect fast and reliable internet speeds, and any issues with speed can impact their ability to work, stream content, or communicate with others. We take speed and reliability very seriously and invest heavily in our network infrastructure to ensure that our customers receive the best possible service. However, we understand that sometimes speed issues can arise due to various factors such as high traffic, network congestion, or technical difficulties. Our customer support team is available 24/7 to address any speed-related concerns and provide solutions to improve connectivity.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmc.invoke(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d700d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = '''[INST] <<SYS>>\n",
    "You are a CEO of telecommunications company<</SYS>>\n",
    "'Is comment that is \"ì¸í„°ë„·ì´ ë¶ˆì•ˆì •í•´\" related to telecommunications company? your answer start Yes or No'[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "84b8dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As the CEO of a telecommunications company, I can confirm that \"ì¸í„°ë„·ì´ ë¶ˆì•ˆì •í•´\" (internet is unstable) is indeed related to our industry. The stability and reliability of the internet are crucial for our customers to access and use our services, such as mobile data, fixed broadband, and other digital communication platforms.\n",
      "\n",
      "Therefore, I would answer \"Yes\" to your question."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  As the CEO of a telecommunications company, I can confirm that \"ì¸í„°ë„·ì´ ë¶ˆì•ˆì •í•´\" (internet is unstable) is indeed related to our industry. The stability and reliability of the internet are crucial for our customers to access and use our services, such as mobile data, fixed broadband, and other digital communication platforms.\\n\\nTherefore, I would answer \"Yes\" to your question.'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmc.invoke(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8052210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = '''[INST] <<SYS>>\n",
    "You are a CEO of telecommunications company<</SYS>>\n",
    "'Is comment that is \"ì»¤í”¼ê°€ ë§›ìˆë„¤ìš”\" related to telecommunications company? your answer must start with Yes or No'[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6310be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No, the comment \"ì»¤í”¼ê°€ ë§›ìˆë„¤ìš”\" (which means \"The coffee tastes good\") is not directly related to a telecommunications company. As a CEO of a telecommunications company, I would be more focused on topics such as network coverage, data speeds, and customer service rather than coffee taste."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  No, the comment \"ì»¤í”¼ê°€ ë§›ìˆë„¤ìš”\" (which means \"The coffee tastes good\") is not directly related to a telecommunications company. As a CEO of a telecommunications company, I would be more focused on topics such as network coverage, data speeds, and customer service rather than coffee taste.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmc.invoke(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "691c2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = '''[INST] <<SYS>>\n",
    "You are a CEO of telecommunications company<</SYS>>\n",
    "'Is comment that is \"KT ì¸í„°ë„· ì¢‹ìŒ?\" related to telecommunications company? your answer must start with Yes or No'[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a020f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yes, the comment \"KT ì¸í„°ë„· ì¢‹ìŒ?\" is related to a telecommunications company. KT is a South Korean telecommunications company, and \"ì¸í„°ë„· ì¢‹ìŒ?\" can be translated to \"Is your internet good?\" in English."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Yes, the comment \"KT ì¸í„°ë„· ì¢‹ìŒ?\" is related to a telecommunications company. KT is a South Korean telecommunications company, and \"ì¸í„°ë„· ì¢‹ìŒ?\" can be translated to \"Is your internet good?\" in English.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmc.invoke(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62dbf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
